# Dev Log: Test Runs & AI Observability

**Version:** v0.2
**Date:** 2026-02-26

## What Happened in This Session

### Task 1: Run Existing Test Case
**Question:** Run `agent.py` with the existing HR test scenario to see the v0.2 report output.
**Outcome:** Hit Windows `UnicodeEncodeError` (GBK codec can't encode emoji). User ran manually with `PYTHONIOENCODING=utf-8` workaround. Report generated successfully — 363 lines, no truncation.
**Observation:** v0.2 report structure differs from v0.1 (no Executive Summary, no Risk Matrix). Identified root cause: analysis prompt only specifies 4 sections — everything else is LLM discretion. Not a token limit issue; v0.2 report is longer and completes naturally.

### Task 2: Add Multiple Test Scenarios
**Question:** Can we store different test scenarios in the code so they're easy to run?
**Solution:** Added `TEST_SCENARIOS` dict with 4 scenarios (hr, healthcare, fintech, education) and a CLI runner. Usage: `python agent.py <scenario>`.
**Files modified:** `agent.py` — replaced `__main__` block

### Task 3: Add Timing and Performance Logging
**Question:** How do I track report generation time? Should I keep a centralized performance log?
**Decision:** Implement lightweight AI observability — timing per pipeline step, embedded in reports, and aggregated in a CSV log.
**Solution (3 changes):**
1. Per-step timing in `run_analysis()` — tracks planning, research, and analysis phases separately using `time.time()`
2. Timing line in report header via `save_report()` — each report is self-documenting
3. Centralized `reports/test-log.csv` via new `append_test_log()` — appends a row after every run with timing, inputs, and report filename
**Files modified:** `agent.py` — added `import time, csv`, timing logic in `run_analysis()`, timing display in `save_report()`, new `append_test_log()` function

### Task 4: Run All Test Scenarios
**Runs completed by user:**
| Scenario | Report Lines | Total Time | Analysis Time | Notes |
|---|---|---|---|---|
| hr | 363 | (pre-timing) | — | Compared to v0.1; structure varies |
| healthcare | 790 | (pre-timing) | — | Longest report; heavy HIPAA |
| fintech | 544 | 121.0s | 102.7s (85%) | FCA + GDPR + EU AI Act |
| education | 266 | 78.3s | 58.4s (75%) | FERPA + COPPA; narrowest scope |

**Key finding:** Analysis phase is the bottleneck (75-85% of total time). Research is consistently ~10s.

### Task 5: Documentation Update
**Question:** Log everything and update DOCUMENTATION-GUIDE.md for AI observability.
**Solution:** Updated iteration doc (test results, new features, new issues), created this dev log, updated CHANGELOG, added Section 4 (AI Observability) to DOCUMENTATION-GUIDE.md with schema, rules, and future growth path.
**Files modified:** `docs/iterations/v0.2-bug-fixes-and-observability.md`, `docs/dev-logs/v0.2_2026-02-26_1200_test-runs-and-observability.md` (this file), `CHANGELOG.md`, `docs/DOCUMENTATION-GUIDE.md`

### Task 6: Iteration Doc Rename & New Documentation Rule
**Question:** The v0.2 iteration doc title ("Bug Fixes") no longer reflects the full scope after Session 2 added observability. Add a rule for this.
**Solution:**
- Added rule to DOCUMENTATION-GUIDE.md Section 1: iteration doc title/filename must be updated when scope expands across sessions
- Renamed `v0.2-bug-fixes.md` → `v0.2-bug-fixes-and-observability.md`
- Updated heading to `# v0.2 — Bug Fixes, Hardening & Observability`
- Updated all references in CHANGELOG, both dev logs, and DOCUMENTATION-GUIDE
**Files modified:** `docs/DOCUMENTATION-GUIDE.md` (new rule + renumbered sections), `docs/iterations/v0.2-bug-fixes-and-observability.md` (renamed + title), `CHANGELOG.md`, both dev log files (link updates)

### Task 7: Final Documentation Consistency Check
**Question:** Run a final check across all v0.2 docs.
**Fixes:** Merged two separate "Medium Priority" tables into one in iteration doc, updated Goal section to reflect both sessions, added missing version log entries for rename/doc updates, added this task to dev log.

### Task 8: Add Commit Message Suggestion Rule
**Question:** Add a rule so the agent suggests a commit message after documentation updates, since committing is typically the next step.
**Solution:** Added to DOCUMENTATION-GUIDE.md Section 7 (Git Commit Messages): new rule that agent should suggest a commit message after completing docs, a template with examples by session type, and updated the Workflow to include "suggest commit message" as step 8 before commit/push.
**Files modified:** `docs/DOCUMENTATION-GUIDE.md`

## Key Decisions Made
1. Don't re-run hr/healthcare just for timing data — save API costs, timing matters more going forward
2. Analysis prompt inconsistency (issue #16) deferred to v0.3 — good enough for Streamlit demo
3. AI observability starts lightweight (CSV log) with intent to grow — user recognized this as a product concern worth building on
4. Windows encoding issue (#17) logged but not fixed in code — workaround documented
5. Iteration doc titles must stay accurate across sessions — new documentation rule added
6. Agent should suggest commit messages after documentation updates — new workflow rule added

## Related Files
- Iteration doc: [docs/iterations/v0.2-bug-fixes-and-observability.md](../iterations/v0.2-bug-fixes-and-observability.md)
- Test log: [reports/test-log.csv](../../reports/test-log.csv)
- Reports generated: `reports/report_v0.2_*.md` (4 files)
- Changelog: [CHANGELOG.md](../../CHANGELOG.md)
