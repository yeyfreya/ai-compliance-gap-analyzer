# v0.2 — Bug Fixes, Hardening & Observability

**Date:** 2026-02-26
**Analyst:** Cursor AI Agent (Claude)
**Source chats:**
- Session 1: Bug fixes & hardening
- Session 2: Test runs & AI observability
- Session 3: Documentation system improvements, README update & repo rename

---

## Goal

**Session 1:** Fix all remaining issues from v0.1 baseline analysis, ordered by priority:
1. **Fix Now** — Live bugs affecting correctness
2. **High** — Missing error handling and validation
3. **Medium** — Prompt improvements, architecture, dependency hygiene (deferred)
4. **Low** — Quality-of-life improvements (deferred)

**Session 2:** Test the hardened pipeline across diverse regulatory scenarios. Add per-step timing and centralized performance logging — the project's first AI observability layer.

Full issue list: [docs/iterations/v0.1-baseline.md](v0.1-baseline.md) → "Remaining Issues for v0.2"

---

## Changes Made During This Version

### Bug #1 — `format_search_results()` type mismatch [Fix Now]
- **File:** `tools.py` lines 76-89
- **Root cause:** `conduct_research()` in `agent.py` extracts the inner list from Tavily's response with `.get('results', [])` and passes that **list** to `format_search_results()`. But the function treated its input as a **dict** — checking for a `'results'` key and indexing with `search_results['results']`. Since lists don't have string keys, the guard clause always triggered, returning "No search results found" even when Tavily returned valid data.
- **Confirmed by:** v0.1 test run — Claude's output noted "search queries didn't return results" despite Tavily working.
- **Fix:** Changed `format_search_results()` to work with the list it actually receives. Removed the dict-style `'results' not in search_results` guard. Now iterates the list directly with `enumerate(search_results, 1)`. The type hint already declared `list`, so the implementation now matches its own contract.
- **Why this approach:** Fix the receiver, not the caller. The caller correctly extracts a clean list — changing it to pass the full dict would leak transport format into the formatter (worse separation of concerns).

### Bug #2 — Inconsistent indentation in `run_analysis()` [Fix Now]
- **File:** `agent.py` lines 199-206
- **Original finding:** Step comments had 5-space indentation instead of 4-space.
- **Status:** Already fixed. Current code has consistent 4-space indentation. Likely corrected during a previous edit session (file shows as modified/staged in git).
- **Action:** No code change needed — closed as already resolved.

### Bug #15 — `max_tokens=3000` too low [Fix Now]
- **File:** `agent.py` line 123
- **Root cause:** `analyze_compliance()` called Claude with `max_tokens=3000`. The v0.1 test run produced a ~280-line analysis with 10 gaps, 15 recommendations, and a risk matrix — but 3000 tokens wasn't enough. The report's Risk Prioritization Matrix was cut off mid-table.
- **Confirmed by:** v0.1 test run — report truncated at the end.
- **Fix:** Increased `max_tokens` from `3000` to `8000`. This is a ceiling, not a minimum — Claude still stops when its response is naturally complete. 8000 is generous enough for detailed analyses while staying well under Claude's context limit.
- **Why this approach:** Single-value change with minimal risk. The `plan_searches()` call keeps `max_tokens=1000` (search query lists are short). Only the analysis call needed the bump since that's the one producing long-form output.

### Bug #3 — No error handling on Claude API calls [High]
- **File:** `agent.py` — `plan_searches()` (lines 39-46) and `analyze_compliance()` (lines 121-128)
- **Root cause:** Both Claude API calls had zero error handling. If the API fails (rate limit, network timeout, invalid key, Anthropic outage), the entire pipeline crashes with a raw traceback. For `analyze_compliance()`, this also wastes all Tavily API credits already spent on research.
- **Fix:** Wrapped both API calls in `try/except anthropic.APIError`. `plan_searches()` falls through to its existing fallback queries so the pipeline continues. `analyze_compliance()` returns a clear error string that gets saved into the report, preserving research data for retry.
- **Why this approach:** Catches `anthropic.APIError` (SDK base class) rather than bare `Exception` — covers rate limits, auth, timeouts, server errors without swallowing unrelated exceptions. Errors handled in the core layer so any frontend (Streamlit, Flask, OpenClaw plugin) gets predictable behavior from `run_analysis()`.

### Bug #4 — No input validation on `run_analysis()` parameters [High]
- **File:** `agent.py` — `run_analysis()` entry point
- **Root cause:** Empty strings, whitespace-only, or extremely long inputs passed straight through to Claude and Tavily unchecked — burning API credits for nothing and producing reports with blank metadata.
- **Fix:** Added validation at the top of `run_analysis()` that checks all three inputs (`use_case`, `technology`, `industry`). Returns `{"error": "..."}` dict immediately if a field is empty/whitespace or exceeds 500 chars. No API calls made on invalid input.
- **Why this approach:** Returns an error dict (same shape check as normal result) rather than raising an exception — any frontend can check `if "error" in result:` and display accordingly. Validated in the core so it works regardless of frontend. 500-char limit prevents accidental token burning from copy-paste mistakes.

### Bug #6 — Bare `except` in `plan_searches()` [High]
- **File:** `agent.py` — `plan_searches()` JSON parsing fallback
- **Original finding:** Bare `except` clause silently swallowing all exceptions including `KeyboardInterrupt` and `MemoryError`.
- **Status:** Already fixed. Current code catches `(json.JSONDecodeError, ValueError, IndexError)` specifically. Corrected in a prior session (file was modified/staged in git).
- **Action:** No code change needed — closed as already resolved.

---

## Changes Made During This Version (Session 2 — Test Runs & Observability)

### Test scenario runner in `agent.py`
- **File:** `agent.py` — `TEST_SCENARIOS` dict + updated `__main__` block
- **What:** Replaced single hardcoded test with a `TEST_SCENARIOS` dictionary holding 4 scenarios (hr, healthcare, fintech, education). CLI now accepts `python agent.py <scenario>` to pick which one to run. Running without args prints a usage menu.
- **Why:** Needed to test the pipeline across diverse regulatory domains before building the Streamlit UI. Reusable for future regression testing.

### Per-step timing in `run_analysis()`
- **File:** `agent.py` — `run_analysis()` function
- **What:** Added `time.time()` instrumentation around each pipeline step (planning, research, analysis). Timing dict included in the result and printed to console on completion.
- **Why:** Track agent performance for product/UX decisions — e.g., is research or analysis the bottleneck? Essential for optimizing user wait times in the Streamlit UI.

### Timing in saved reports
- **File:** `agent.py` — `save_report()` function
- **What:** Report header now includes a `**Generation Time:**` line showing total and per-step durations.
- **Why:** Each report is self-documenting — anyone reviewing a report can see how long it took without checking external logs.

### Centralized test log (`reports/test-log.csv`)
- **File:** `agent.py` — new `append_test_log()` function
- **What:** Every successful `run_analysis()` call appends a row to `reports/test-log.csv` with: timestamp, version, inputs, query count, per-step timing, and report filename.
- **Why:** Single source of truth for tracking agent performance across all versions and scenarios. This is the project's first step toward **AI observability** — monitoring latency, throughput, and behavior over time.

---

## Changes Made During This Version (Session 3 — Docs, README & Rename)

### Added README update rules to documentation system
- **File:** `docs/DOCUMENTATION-GUIDE.md` — new Section 7 (README.md)
- **What:** Defined what the README contains, a trigger table for when to update it, and rules for keeping it current. Added README update to the version workflow (step 6) and the AI quick reference checklist.
- **File:** `.cursor/rules/documentation-system.mdc` — added README update reminder to "After completing work" checklist
- **Why:** README was getting stale because no rule enforced updating it. Now it's part of the standard workflow.

### Synced Cursor rules with documentation guide
- **File:** `.cursor/rules/documentation-system.mdc`
- **What:** Fixed dev log naming format to include `<HHMM>` timestamp. Added "suggest a commit message" step.
- **Why:** Cursor rule had drifted out of sync with the guide — naming format was missing the time component, and the commit suggestion step wasn't surfaced.

### Created local-only documentation system map
- **File:** `docs/DOCUMENTATION-SYSTEM-MAP.md` (gitignored)
- **What:** Visual reference file mapping the entire documentation system: who creates/updates each document, session lifecycle flow, how documents connect to each other, version bump checklist, and naming conventions.
- **Why:** Documentation system was growing complex enough to need its own reference guide, separate from the prescriptive DOCUMENTATION-GUIDE.md.

### Updated README.md to v0.2 current state
- **File:** `README.md`
- **What:** Bumped status to v0.2. Updated architecture diagram (added `append_test_log`). Updated project structure (added `test-log.csv`, renamed root to `ai-compliance-gap-analyzer/`). Rewrote Usage section with CLI scenario commands. Updated Known Issues from v0.1 to v0.2. Checked off 4 roadmap items. Updated current version link.
- **Why:** README was still showing v0.1 state — all v0.2 work was invisible to anyone landing on the repo.

### Renamed repo to `ai-compliance-gap-analyzer`
- **Files:** `README.md` (clone URL, cd, structure tree), `docs/DOCUMENTATION-GUIDE.md` (structure tree)
- **What:** Updated all current-state references from `compliance-gap-analyzer` to `ai-compliance-gap-analyzer`. Historical references in dev logs and transcripts left unchanged.
- **Why:** User wanted the repo name to reflect that this is an AI-powered tool. Rename done on GitHub; internal references updated to match.

---

## Test Results

### Test Run 1 — HR (existing scenario, pre-timing)
- **Scenario:** `hr` — AI-powered resume screening tool / OpenAI GPT-4 API / Enterprise HR tech (US-based)
- **Report:** `reports/report_v0.2_ai-powered-resume-screening-tool_20260226_120943.md` (363 lines)
- **Timing:** Not recorded (run before timing feature was added)
- **Outcome:** Full report generated successfully. All 4 sections present (Regulations, Risks, Gaps, Recommendations) plus Critical Risk Summary and Resources. No truncation. Compared to v0.1 report — different structure (no Executive Summary, no Risk Matrix) due to non-deterministic LLM output and loosely specified analysis prompt.

### Test Run 2 — Healthcare (pre-timing)
- **Scenario:** `healthcare` — AI diagnostic assistant / Google Gemini API / US hospital network
- **Report:** `reports/report_v0.2_ai-diagnostic-assistant-that-analyzes-medical-images-and-suggests-diagnoses_20260226_122310.md` (790 lines)
- **Timing:** Not recorded (run before timing feature was added)
- **Outcome:** Longest report produced so far (790 lines). Heavy HIPAA coverage as expected. Report completed without truncation.

### Test Run 3 — Fintech
- **Scenario:** `fintech` — AI credit scoring / AWS SageMaker / UK neobank
- **Report:** `reports/report_v0.2_ai-credit-scoring-model-that-evaluates-loan-applications_20260226_123407.md` (544 lines)
- **Timing:** 121.0s total (planning: 7.7s, research: 10.6s, analysis: 102.7s)
- **Outcome:** First run with timing. Analysis phase dominates at 85% of total time. Report covers FCA, GDPR, EU AI Act. No truncation.

### Test Run 4 — Education
- **Scenario:** `education` — AI essay grading / Anthropic Claude API / US K-12
- **Report:** `reports/report_v0.2_ai-essay-grading-and-feedback-tool-for-student-assignments_20260226_124125.md` (266 lines)
- **Timing:** 78.3s total (planning: 9.4s, research: 10.5s, analysis: 58.4s)
- **Outcome:** Shortest report (266 lines). Covers FERPA, COPPA, state student privacy laws. Faster analysis likely due to narrower regulatory scope. No truncation.

### Observations Across All Runs
- **All 4 scenarios completed successfully** — no crashes, no API errors, no truncation
- **Report length varies widely** (266–790 lines) depending on regulatory complexity
- **Analysis is the bottleneck** — 75-85% of total time in both timed runs
- **Research is fast** — ~10s consistently across scenarios (5 Tavily searches each)
- **Report structure varies between runs** — analysis prompt (issue #10) doesn't enforce a fixed template. Flagged for v0.3.
- **Windows encoding note:** Running `python agent.py` directly on Windows may hit `UnicodeEncodeError` for emoji characters with GBK codec. Workaround: set `PYTHONIOENCODING=utf-8` before running.

---

## Remaining Issues for v0.3 (Deferred to Post-Launch)

Decision: Medium and Low priority bugs deferred to focus on Streamlit demo launch. The core pipeline is now correct and hardened — remaining items are quality/polish improvements.

### Medium Priority
| # | Issue | Source |
|---|---|---|
| 9 | System prompt misleading about tool access | Code analysis |
| 10 | Analysis prompt could request severity ratings, compliance scores | Code analysis |
| 7 | Pipeline → agent loop (add research adequacy check) | Code analysis |
| 11 | `requirements.txt` has no version pins | Code analysis |
| 16 | Analysis prompt doesn't enforce consistent report structure (no fixed template) | Test runs — structure varies between runs |
| 17 | Windows GBK encoding error on emoji output | Test run — `UnicodeEncodeError` on `python agent.py` |

### Low Priority
| # | Issue | Source |
|---|---|---|
| 5 | Retry logic for API calls | Code analysis |
| 8 | Multi-turn Claude context (stateless calls) | Code analysis |
| 12 | Remove unused `streamlit` from requirements (moot once Streamlit app is built) | Code analysis |
| 13 | Pre-process research text before sending to Claude | Code analysis |
| 14 | Include raw research sources in saved report | Code analysis |

---

## Version Log
- **2026-02-26** — v0.2 started, iteration doc created
- **2026-02-26** — Bug #1 fixed: `format_search_results()` type mismatch in `tools.py`
- **2026-02-26** — Bug #2 closed: indentation already fixed in prior session
- **2026-02-26** — Bug #15 fixed: `max_tokens` increased from 3000 to 8000 in `agent.py`
- **2026-02-26** — Bug #3 fixed: error handling added to both Claude API calls in `agent.py`
- **2026-02-26** — Bug #4 fixed: input validation added to `run_analysis()` in `agent.py`
- **2026-02-26** — Bug #6 closed: bare except already fixed in prior session
- **2026-02-26** — Added `TEST_SCENARIOS` dict and CLI scenario runner to `agent.py`
- **2026-02-26** — Added per-step timing to `run_analysis()` and report header
- **2026-02-26** — Added `append_test_log()` and centralized `reports/test-log.csv`
- **2026-02-26** — Test runs completed: hr, healthcare, fintech, education — all 4 passed
- **2026-02-26** — New issues found: #16 (inconsistent report structure), #17 (Windows encoding)
- **2026-02-26** — Added doc rule: iteration doc title/filename must be updated when scope expands across sessions
- **2026-02-26** — Renamed `v0.2-bug-fixes.md` → `v0.2-bug-fixes-and-observability.md`; updated all references
- **2026-02-26** — Added Section 4 (AI Observability) to `DOCUMENTATION-GUIDE.md`
- **2026-02-26** — Added Section 7 (README.md update rules) to `DOCUMENTATION-GUIDE.md`; renumbered sections 7→10
- **2026-02-26** — Synced `.cursor/rules/documentation-system.mdc` with guide (HHMM format, commit suggestion step)
- **2026-02-26** — Created `docs/DOCUMENTATION-SYSTEM-MAP.md` (local-only, gitignored) — visual documentation reference
- **2026-02-26** — Updated `README.md` to v0.2: status, architecture, structure, usage, known issues, roadmap, current version
- **2026-02-26** — Renamed repo to `ai-compliance-gap-analyzer`; updated clone URL and structure trees
